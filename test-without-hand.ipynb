{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11496442,"sourceType":"datasetVersion","datasetId":7206894},{"sourceId":11496457,"sourceType":"datasetVersion","datasetId":7206906}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#finding the best test accuracy\n#without handcoded features\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom torch.utils.data import Subset\nimport wandb\nfrom PIL import Image\nfrom tabulate import tabulate\nimport numpy as np\nimport random\nimport shutil\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:02:59.945053Z","iopub.execute_input":"2025-05-04T21:02:59.945581Z","iopub.status.idle":"2025-05-04T21:03:15.287544Z","shell.execute_reply.started":"2025-05-04T21:02:59.945551Z","shell.execute_reply":"2025-05-04T21:03:15.286788Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Adapter so FineTuneCNN still gets (img, tab, label)\nclass ImageOnlyDataset(Dataset):\n    def __init__(self, ds):\n        self.ds = ds\n    def __len__(self):\n        return len(self.ds)\n    def __getitem__(self, idx):\n        img, lbl = self.ds[idx]\n        tab = torch.zeros(0, dtype=torch.float32)\n        return img, tab, lbl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:03:20.005795Z","iopub.execute_input":"2025-05-04T21:03:20.006324Z","iopub.status.idle":"2025-05-04T21:03:20.010832Z","shell.execute_reply.started":"2025-05-04T21:03:20.006297Z","shell.execute_reply":"2025-05-04T21:03:20.010209Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"IMG_SIZE = (224, 224)\ntransform_pipeline = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\n\n# Paths\nimage_root      = '/kaggle/input/train-valid/train'  # train+val\ntest_image_root = '/kaggle/input/test-data'                                # fixed test\n\nrandom.seed(42)\n\n#  Build the full train+val dataset\nfull_trainval = datasets.ImageFolder(root=image_root, transform=transform_pipeline)\ntotal_tv      = len(full_trainval)\n# 90% train, 10% val\nn_val   = int(0.1 * total_tv)\nn_train = total_tv - n_val\n\ntrain_ds, val_ds = random_split(\n    full_trainval,\n    [n_train, n_val],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Load the fixed test dataset (no split)\ntest_ds = datasets.ImageFolder(\n    root=test_image_root,\n    transform=transform_pipeline\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:03:22.571497Z","iopub.execute_input":"2025-05-04T21:03:22.571743Z","iopub.status.idle":"2025-05-04T21:03:50.492897Z","shell.execute_reply.started":"2025-05-04T21:03:22.571725Z","shell.execute_reply":"2025-05-04T21:03:50.492356Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#print the details of dataset\n\ntotal   = total_tv\nn_train = len(train_ds)\nn_val   = len(val_ds)\nn_test  = len(test_ds)\n\n# class counts in the ORIGINAL image_root\n# A) Print per‑class counts for the fixed test split\nclass_counts = []\nfor cls in sorted(os.listdir(test_image_root)):\n    cls_dir = os.path.join(test_image_root, cls)\n    if os.path.isdir(cls_dir):\n        cnt = len([f for f in os.listdir(cls_dir)\n                   if f.lower().endswith(('.png','.jpg','.jpeg'))])\n        class_counts.append((cls, cnt))\nprint(tabulate(class_counts, headers=['Class', '# Test Images']))\ntotal_test = sum(c for _, c in class_counts)\nprint(f\"\\nTotal test images: {total_test}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:03:55.575475Z","iopub.execute_input":"2025-05-04T21:03:55.575742Z","iopub.status.idle":"2025-05-04T21:03:55.597918Z","shell.execute_reply.started":"2025-05-04T21:03:55.575720Z","shell.execute_reply":"2025-05-04T21:03:55.597255Z"}},"outputs":[{"name":"stdout","text":"Class                                                        # Test Images\n---------------------------------------------------------  ---------------\n1. Eczema                                                              335\n10. Warts Molluscum and other Viral Infections                         420\n2. Melanoma                                                            628\n3. Atopic Dermatitis                                                   251\n4. Basal Cell Carcinoma                                                664\n5. Melanocytic Nevi                                                   1594\n6. Benign Keratosis-like Lesions                                       415\n7. Psoriasis pictures Lichen Planus and related diseases               411\n8. Seborrheic Keratoses and other Benign Tumors                        369\n9. Tinea Ringworm Candidiasis and other Fungal Infections              340\n\nTotal test images: 5427\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#EXTEND ResNet TO CONCAT TABULAR FEATURES \nclass ResNetWithTabular(nn.Module):\n    def __init__(self, base_model, tab_dim, num_classes=10):\n        super().__init__()\n        # everything except the final fc\n        self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n        # only apply BN if we actually have tab features\n        self.tab_bn   = nn.Identity() if tab_dim == 0 else nn.BatchNorm1d(tab_dim)\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(base_model.fc.in_features + tab_dim, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n    def forward(self, img, tab):\n        x = self.backbone(img).flatten(1)  # [B,2048]\n        tab = self.tab_bn(tab)             # Identity for tab_dim=0\n        x = torch.cat([x, tab], dim=1)     # works even if tab has shape [B,0]\n        return self.classifier(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:00.837941Z","iopub.execute_input":"2025-05-04T21:04:00.838266Z","iopub.status.idle":"2025-05-04T21:04:00.845023Z","shell.execute_reply.started":"2025-05-04T21:04:00.838244Z","shell.execute_reply":"2025-05-04T21:04:00.844132Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# FINE‑TUNING CLASS \nclass FineTuneCNN:\n    def __init__(self, train_ds, valid_ds, base_model, batch_size=32, freeze_ratio=1.0, test_ds=None):\n        self.model = base_model\n        # freeze parameters\n        if freeze_ratio >= 1.0:\n            for p in self.model.parameters():\n                p.requires_grad = False\n        else:\n            total_p = sum(1 for _ in self.model.parameters())\n            to_freeze = int(total_p * freeze_ratio)\n            cnt = 0\n            for p in self.model.parameters():\n                p.requires_grad = False\n                cnt += 1\n                if cnt >= to_freeze:\n                    break\n        # ensure final layers are trainable\n        for p in self.model.classifier.parameters():\n            p.requires_grad = True\n\n        # override the default loaders\n        self.train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        self.valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n        self.test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False) if test_ds else None\n\n    def run_training(self, num_epochs=10, learning_rate=1e-3, weight_decay_val=0):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(),\n                               lr=learning_rate,\n                               weight_decay=weight_decay_val)\n\n        for epoch in range(1, num_epochs+1):\n            # --- train ---\n            self.model.train()\n            running_correct = 0\n            running_total   = 0\n            for img, tab, lbl in self.train_loader:\n                img, tab, lbl = img.to(device), tab.to(device), lbl.to(device)\n                optimizer.zero_grad()\n                out = self.model(img, tab)\n                loss = criterion(out, lbl)\n                loss.backward()\n                optimizer.step()\n                pred = out.argmax(dim=1)\n                running_correct += (pred == lbl).sum().item()\n                running_total   += lbl.size(0)\n            train_acc = 100 * running_correct / running_total\n            print(f\"Epoch {epoch} — Train Acc: {train_acc:.2f}%\")\n            wandb.log({\"epoch\":epoch, \"train_acc\":train_acc})\n\n            # --- validate ---\n            self.model.eval()\n            val_corr = 0\n            val_tot  = 0\n            val_loss = 0.0\n            with torch.no_grad():\n                for img, tab, lbl in self.valid_loader:\n                    img, tab, lbl = img.to(device), tab.to(device), lbl.to(device)\n                    out = self.model(img, tab)\n                    l  = criterion(out, lbl)\n                    pred = out.argmax(dim=1)\n                    val_corr += (pred == lbl).sum().item()\n                    val_tot  += lbl.size(0)\n                    val_loss  = l.item()\n            val_acc = 100 * val_corr / val_tot\n            print(f\"Epoch {epoch} — Val   Acc: {val_acc:.2f}%\")\n            wandb.log({\"validation_accuracy\": val_acc, \"validation_loss\": val_loss})\n\n    def evaluate_test(self):\n        if self.test_loader is None:\n            print(\"No test set.\")\n            return\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.eval()\n        test_corr = 0\n        test_tot  = 0\n        with torch.no_grad():\n            for img, tab, lbl in self.test_loader:\n                img, tab, lbl = img.to(device), tab.to(device), lbl.to(device)\n                out = self.model(img, tab)\n                pred = out.argmax(dim=1)\n                test_corr += (pred == lbl).sum().item()\n                test_tot  += lbl.size(0)\n        test_acc = 100 * test_corr / test_tot\n        print(f\"Test Accuracy: {test_acc:.2f}%\")\n        wandb.log({\"test_accuracy\": test_acc})\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:04.760835Z","iopub.execute_input":"2025-05-04T21:04:04.761137Z","iopub.status.idle":"2025-05-04T21:04:04.774554Z","shell.execute_reply.started":"2025-05-04T21:04:04.761116Z","shell.execute_reply":"2025-05-04T21:04:04.773945Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import wandb\nimport numpy as np\nfrom types import SimpleNamespace\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:12.684408Z","iopub.execute_input":"2025-05-04T21:04:12.684673Z","iopub.status.idle":"2025-05-04T21:04:12.688242Z","shell.execute_reply.started":"2025-05-04T21:04:12.684654Z","shell.execute_reply":"2025-05-04T21:04:12.687672Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"wandb.login(key='1df7a902fa4a610500b8e79e21818419d5facdbb')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:15.313792Z","iopub.execute_input":"2025-05-04T21:04:15.314492Z","iopub.status.idle":"2025-05-04T21:04:21.095512Z","shell.execute_reply.started":"2025-05-04T21:04:15.314470Z","shell.execute_reply":"2025-05-04T21:04:21.094968Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m018\u001b[0m (\u001b[33mma23m018-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"BEST_VAL_CONFIGS = [\n    {\n        'learning_rate': 1e-4,\n        'freeze_ratio':  0.2,\n        'l2_reg':        0,\n        'batch_size':    64,\n        'epochs':        10\n    }\n    \n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:25.479155Z","iopub.execute_input":"2025-05-04T21:04:25.480012Z","iopub.status.idle":"2025-05-04T21:04:25.483444Z","shell.execute_reply.started":"2025-05-04T21:04:25.479986Z","shell.execute_reply":"2025-05-04T21:04:25.482789Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# SWEEP CONFIG TO MAXIMIZE test_accuracy \nsweep_config = {\n    'method':  'bayes',\n    'metric':  {'name': 'test_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'config_idx': {'values': list(range(len(BEST_VAL_CONFIGS)))}\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:30.813277Z","iopub.execute_input":"2025-05-04T21:04:30.813929Z","iopub.status.idle":"2025-05-04T21:04:30.818503Z","shell.execute_reply.started":"2025-05-04T21:04:30.813901Z","shell.execute_reply":"2025-05-04T21:04:30.817624Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, entity= \"ma23m018-indian-institute-of-technology-madras\", project=\"mtech_project_wh1test_new\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:35.298802Z","iopub.execute_input":"2025-05-04T21:04:35.299142Z","iopub.status.idle":"2025-05-04T21:04:35.617233Z","shell.execute_reply.started":"2025-05-04T21:04:35.299121Z","shell.execute_reply":"2025-05-04T21:04:35.616592Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: jkrp2zap\nSweep URL: https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/sweeps/jkrp2zap\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#  SWEEP MAIN \ndef main():\n    with wandb.init() as run:\n        idx = run.config.config_idx\n        cfg = BEST_VAL_CONFIGS[idx]\n        # lock in these hyperparameters\n        run.config.update(cfg, allow_val_change=False)\n        run.name = (\n            f\"bs{cfg['batch_size']}\"\n            f\"_ep{cfg['epochs']}\"\n            f\"_lr{cfg['learning_rate']}\"\n            f\"_fr{cfg['freeze_ratio']}\"\n        )\n\n        #\n        train_adapter = ImageOnlyDataset(train_ds)\n        val_adapter   = ImageOnlyDataset(val_ds)\n        test_adapter  = ImageOnlyDataset(test_ds)\n\n        # build & freeze ResNet50\n        base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n        in_feats   = base_model.fc.in_features\n        base_model.fc = torch.nn.Linear(in_feats, 10)\n\n        \n        model = ResNetWithTabular(base_model=base_model, tab_dim=0, num_classes=10)\n\n        # initialize fine‑tuner with test_ds to log test_accuracy\n        finetuner = FineTuneCNN(\n            train_ds=train_adapter,\n            valid_ds=val_adapter,\n            base_model=model,\n            batch_size=cfg['batch_size'],\n            freeze_ratio=cfg['freeze_ratio'],\n            test_ds=test_adapter\n        )\n\n        # train on train+val, then evaluate on test\n        finetuner.run_training(\n            num_epochs=cfg['epochs'],\n            learning_rate=cfg['learning_rate'],\n            weight_decay_val=cfg['l2_reg']\n        )\n        finetuner.evaluate_test()\n\n# LAUNCH SWEEP AGENT\nwandb.agent(sweep_id, function=main, count=len(BEST_VAL_CONFIGS))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T21:04:39.737269Z","iopub.execute_input":"2025-05-04T21:04:39.737843Z","iopub.status.idle":"2025-05-04T21:45:59.007679Z","shell.execute_reply.started":"2025-05-04T21:04:39.737819Z","shell.execute_reply":"2025-05-04T21:45:59.007136Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b8p1xcvg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tconfig_idx: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_210445-b8p1xcvg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/runs/b8p1xcvg' target=\"_blank\">apricot-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/sweeps/jkrp2zap' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/sweeps/jkrp2zap</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/sweeps/jkrp2zap' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/sweeps/jkrp2zap</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/runs/b8p1xcvg' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/runs/b8p1xcvg</a>"},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 — Train Acc: 58.78%\nEpoch 1 — Val   Acc: 68.28%\nEpoch 2 — Train Acc: 71.64%\nEpoch 2 — Val   Acc: 69.98%\nEpoch 3 — Train Acc: 78.88%\nEpoch 3 — Val   Acc: 71.13%\nEpoch 4 — Train Acc: 86.30%\nEpoch 4 — Val   Acc: 68.28%\nEpoch 5 — Train Acc: 91.76%\nEpoch 5 — Val   Acc: 69.61%\nEpoch 6 — Train Acc: 94.24%\nEpoch 6 — Val   Acc: 70.40%\nEpoch 7 — Train Acc: 95.79%\nEpoch 7 — Val   Acc: 69.29%\nEpoch 8 — Train Acc: 96.45%\nEpoch 8 — Val   Acc: 72.38%\nEpoch 9 — Train Acc: 97.43%\nEpoch 9 — Val   Acc: 70.44%\nEpoch 10 — Train Acc: 96.91%\nEpoch 10 — Val   Acc: 70.63%\nTest Accuracy: 70.85%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▄▆▁▃▅▃█▅▅</td></tr><tr><td>validation_loss</td><td>▁▂▁▅▆▇█▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>70.84946</td></tr><tr><td>train_acc</td><td>96.91112</td></tr><tr><td>validation_accuracy</td><td>70.62615</td></tr><tr><td>validation_loss</td><td>1.5521</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bs64_ep10_lr0.0001_fr0.2</strong> at: <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/runs/b8p1xcvg' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new/runs/b8p1xcvg</a><br> View project at: <a href='https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new' target=\"_blank\">https://wandb.ai/ma23m018-indian-institute-of-technology-madras/mtech_project_wh1test_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250504_210445-b8p1xcvg/logs</code>"},"metadata":{}}],"execution_count":12}]}